experiment_name: run_dae
learning_rate: 0.00002
train_batch_size: 8
eval_batch_size: 1
gradient_accumulation_steps: 64
seq_length: 2048
load_in_8bit: False
load_in_4bit: False
use_peft: False
gradient_checkpointing: True
peft_lora_r: 256
peft_lora_alpha: 128
logging_steps: 1
num_train_epochs: 2
max_steps: -1
local_rank: -1
resume_from_checkpoint: False
description: ""
random_seed: 44
weight_decay: 0
warmup_ratio: 0.1
instruction_key: "[INST]"
response_key: "[/INST]"
training_data_path: ""
eval_data_path: ""
base_model_path: ""
target_model_path: ""
domain_model_path: ""
output_dir: ""
deepspeed: ""
lambda_param: 1
temperature: 1